{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters optimization with keras-tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to optimize the parameters of a neural network using keras-tuner. The problem is to predict gas flow based on the temperature and time of the day.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements for keras-tuner installation:\n",
    "\n",
    "    Python 3.6\n",
    "    TensorFlow 2.0\n",
    "    \n",
    "website: https://keras-team.github.io/keras-tuner/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.python.keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "from tensorflow.python.keras import utils, layers, Sequential\n",
    "\n",
    "import kerastuner\n",
    "from kerastuner import RandomSearch\n",
    "from kerastuner import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Read and prepare input data.\n",
    "\n",
    "    The data will be split as follow:\n",
    "        • X the training and testing instances of the dataset.\n",
    "        • Y the label for the dataset instances.\n",
    "\n",
    "    :return: four numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    df = pd.read_csv(\"file2.csv\", sep=',')\n",
    "    train, test = train_test_split(df, test_size=0.2)\n",
    "    train_y = train['PUN_n']\n",
    "    train_x = train.drop(['PUN_n'], axis=1)\n",
    "    test_y = test['PUN_n']\n",
    "    test_x = test.drop(['PUN_n'], axis=1)\n",
    "    train_x = train_x.values\n",
    "    test_x = test_x.values\n",
    "\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "train_x, test_x, train_y, test_y = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create a model which takes an argument hp which is the space of hyperparameters and returns a compiled model. The compilation step is where the optimizer along with the loss function and the metric are defined. \n",
    "\n",
    "In Keras Tuner, hyperparameters have a type (possibilities are Float, Int, Boolean, and Choice) and a unique name. Then, a set of options to help guide the search need to be set:\n",
    "\n",
    "- a minimal, a maximal and a default value for the Float and the Int types\n",
    "- a set of possible values for the Choice type\n",
    "- optionally, a sampling method within linear, log or reversed log. Setting this parameter allows to add prior knowledge you might have about the tuned parameter. We'll see in the next section how it can be used to tune the learning rate for instance\n",
    "- optionally, a step value, i.e the minimal step between two hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hp):\n",
    "    node1 = hp.Choice('node1', values=[29,  33,  35,  50, 104])\n",
    "    node2 = hp.Choice('node2', values=[29,  33,  35,  50, 104])\n",
    "    dropout1 = hp.Choice('dropout1', values=[0.34, 0.35, 0.42, 0.44, 0.79])\n",
    "    dropout2 = hp.Choice('dropout2', values=[0.26, 0.48, 0.52, 0.71, 0.79])\n",
    "    activation1 = hp.Choice('activation1', values=['tanh', 'softmax', 'sigmoid', 'relu'])\n",
    "    activation2 = hp.Choice('activation2', values=['tanh', 'softmax', 'sigmoid', 'relu'])\n",
    "    activation3 = hp.Choice('activation3', values=['tanh', 'softmax', 'sigmoid', 'relu'])\n",
    "\n",
    "    def get_optimizer(optimizer, lr):\n",
    "        if optimizer == 'adam':\n",
    "            return tensorflow.keras.optimizers.Adam(learning_rate=lr)\n",
    "        elif optimizer == 'rmsprop':\n",
    "            return tensorflow.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "        elif optimizer == 'sgd':\n",
    "            return tensorflow.keras.optimizers.SGD(learning_rate=lr)\n",
    "\n",
    "    lr = hp.Choice('lr', values=[0.0011, 0.0331, 0.0464, 0.0654, 0.0926])\n",
    "    optimizer = hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd'])\n",
    "\n",
    "    optimizer = get_optimizer(optimizer, lr)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(node1, activation=activation1, input_shape=(train_x.shape[1],)))\n",
    "    model.add(Dropout(rate=dropout1))\n",
    "    model.add(Dense(node2, activation=activation2))\n",
    "    model.add(Dropout(rate=dropout2))\n",
    "    model.add(Dense(1, activation=activation3))\n",
    "\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, instantiate a tuner to determine which hyperparameter combinations should be tested. You should specify the model-building function, the name of the objective to optimize (whether to minimize or maximize is automatically inferred for built-in metrics), the total number of trials (max_trials) to test, and the number of models that should be built and fit for each trial (executions_per_trial). Available tuners are RandomSearch and Hyperband. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mpich\\anaconda3\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 105, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-9-0d7f17a7051c>\", line 21, in create_model\n",
      "    optimizer = get_optimizer(optimizer, lr)\n",
      "  File \"<ipython-input-9-0d7f17a7051c>\", line 12, in get_optimizer\n",
      "    return tensorflow.keras.optimizers.Adam(learning_rate=lr)\n",
      "  File \"C:\\Users\\mpich\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\", line 471, in __init__\n",
      "    super(Adam, self).__init__(**kwargs)\n",
      "  File \"C:\\Users\\mpich\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\", line 68, in __init__\n",
      "    'passed to optimizer: ' + str(k))\n",
      "TypeError: Unexpected keyword argument passed to optimizer: learning_rate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:yellow\">[Warning] Invalid model 0/5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mpich\\anaconda3\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 105, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-9-0d7f17a7051c>\", line 21, in create_model\n",
      "    optimizer = get_optimizer(optimizer, lr)\n",
      "  File \"<ipython-input-9-0d7f17a7051c>\", line 12, in get_optimizer\n",
      "    return tensorflow.keras.optimizers.Adam(learning_rate=lr)\n",
      "  File \"C:\\Users\\mpich\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\", line 471, in __init__\n",
      "    super(Adam, self).__init__(**kwargs)\n",
      "  File \"C:\\Users\\mpich\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\", line 68, in __init__\n",
      "    'passed to optimizer: ' + str(k))\n",
      "TypeError: Unexpected keyword argument passed to optimizer: learning_rate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:yellow\">[Warning] Invalid model 1/5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mpich\\anaconda3\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 105, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-9-0d7f17a7051c>\", line 21, in create_model\n",
      "    optimizer = get_optimizer(optimizer, lr)\n",
      "  File \"<ipython-input-9-0d7f17a7051c>\", line 12, in get_optimizer\n",
      "    return tensorflow.keras.optimizers.Adam(learning_rate=lr)\n",
      "  File \"C:\\Users\\mpich\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\", line 471, in __init__\n",
      "    super(Adam, self).__init__(**kwargs)\n",
      "  File \"C:\\Users\\mpich\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\", line 68, in __init__\n",
      "    'passed to optimizer: ' + str(k))\n",
      "TypeError: Unexpected keyword argument passed to optimizer: learning_rate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:yellow\">[Warning] Invalid model 2/5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mpich\\anaconda3\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 105, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-9-0d7f17a7051c>\", line 21, in create_model\n",
      "    optimizer = get_optimizer(optimizer, lr)\n",
      "  File \"<ipython-input-9-0d7f17a7051c>\", line 12, in get_optimizer\n",
      "    return tensorflow.keras.optimizers.Adam(learning_rate=lr)\n",
      "  File \"C:\\Users\\mpich\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\", line 471, in __init__\n",
      "    super(Adam, self).__init__(**kwargs)\n",
      "  File \"C:\\Users\\mpich\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\", line 68, in __init__\n",
      "    'passed to optimizer: ' + str(k))\n",
      "TypeError: Unexpected keyword argument passed to optimizer: learning_rate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:yellow\">[Warning] Invalid model 3/5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mpich\\anaconda3\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 105, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-9-0d7f17a7051c>\", line 21, in create_model\n",
      "    optimizer = get_optimizer(optimizer, lr)\n",
      "  File \"<ipython-input-9-0d7f17a7051c>\", line 12, in get_optimizer\n",
      "    return tensorflow.keras.optimizers.Adam(learning_rate=lr)\n",
      "  File \"C:\\Users\\mpich\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\", line 471, in __init__\n",
      "    super(Adam, self).__init__(**kwargs)\n",
      "  File \"C:\\Users\\mpich\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\", line 68, in __init__\n",
      "    'passed to optimizer: ' + str(k))\n",
      "TypeError: Unexpected keyword argument passed to optimizer: learning_rate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:yellow\">[Warning] Invalid model 4/5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mpich\\anaconda3\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 105, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-9-0d7f17a7051c>\", line 21, in create_model\n",
      "    optimizer = get_optimizer(optimizer, lr)\n",
      "  File \"<ipython-input-9-0d7f17a7051c>\", line 12, in get_optimizer\n",
      "    return tensorflow.keras.optimizers.Adam(learning_rate=lr)\n",
      "  File \"C:\\Users\\mpich\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\", line 471, in __init__\n",
      "    super(Adam, self).__init__(**kwargs)\n",
      "  File \"C:\\Users\\mpich\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\", line 68, in __init__\n",
      "    'passed to optimizer: ' + str(k))\n",
      "TypeError: Unexpected keyword argument passed to optimizer: learning_rate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:yellow\">[Warning] Invalid model 5/5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Too many failed attempts to build model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, hp)\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mmaybe_distribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-0d7f17a7051c>\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m(hp)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-0d7f17a7051c>\u001b[0m in \u001b[0;36mget_optimizer\u001b[1;34m(optimizer, lr)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'adam'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'rmsprop'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lr, beta_1, beta_2, epsilon, decay, amsgrad, **kwargs)\u001b[0m\n\u001b[0;32m    470\u001b[0m                **kwargs):\n\u001b[1;32m--> 471\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m         raise TypeError('Unexpected keyword argument '\n\u001b[1;32m---> 68\u001b[1;33m                         'passed to optimizer: ' + str(k))\n\u001b[0m\u001b[0;32m     69\u001b[0m       \u001b[1;31m# checks that clipnorm >= 0 and clipvalue >= 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unexpected keyword argument passed to optimizer: learning_rate",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ab60be391491>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m tuner = RandomSearch(create_model, executions_per_trial=3, objective=kerastuner.Objective(\"loss\", direction=\"min\"), \n\u001b[1;32m----> 2\u001b[1;33m                 max_trials = 2, project_name='NN_gas')\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\kerastuner\\tuners\\randomsearch.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, hypermodel, objective, max_trials, seed, hyperparameters, tune_new_entries, allow_new_entries, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0moracle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[0mhypermodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m             **kwargs)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\kerastuner\\engine\\multi_execution_tuner.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, oracle, hypermodel, executions_per_trial, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m                  **kwargs):\n\u001b[0;32m     57\u001b[0m         super(MultiExecutionTuner, self).__init__(\n\u001b[1;32m---> 58\u001b[1;33m             oracle, hypermodel, **kwargs)\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             raise ValueError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\kerastuner\\engine\\tuner.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, oracle, hypermodel, max_model_size, optimizer, loss, metrics, distribution_strategy, directory, project_name, logger, tuner_id, overwrite)\u001b[0m\n\u001b[0;32m    101\u001b[0m                                     \u001b[0mproject_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproject_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                                     \u001b[0mlogger\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                                     overwrite=overwrite)\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution_strategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\kerastuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, oracle, hypermodel, directory, project_name, logger, overwrite)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuner_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_populate_initial_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moverwrite\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tuner_fname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\kerastuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36m_populate_initial_space\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \"\"\"\n\u001b[0;32m    105\u001b[0m         \u001b[0mhp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\u001b[0m in \u001b[0;36m_build_wrapper\u001b[1;34m(self, hp, *args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;31m# to the search space.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mhp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, hp)\u001b[0m\n\u001b[0;32m    113\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_fail_streak\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m                     raise RuntimeError(\n\u001b[1;32m--> 115\u001b[1;33m                         'Too many failed attempts to build model.')\n\u001b[0m\u001b[0;32m    116\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Too many failed attempts to build model."
     ]
    }
   ],
   "source": [
    "tuner = RandomSearch(create_model, executions_per_trial=3, objective=kerastuner.Objective(\"loss\", direction=\"min\"), \n",
    "                max_trials = 2, project_name='NN_gas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three possible algorithms that can be used for the hyperparameters optimization: random searc, bayesian optimization adn Hyperband.\n",
    "    \n",
    "- Random search: randomly tries a combination of hyperparameters from a given search space.\n",
    "  - max_trials variable: number of hyperparameter combinations that will be tested by the tuner\n",
    "  - execution_per_trial variable: the number of models that should be built and fit for each trial for robustness purposes. \n",
    "- Hyperband is an optimized version of random search which uses early-stopping to speed up the hyperparameter tuning process. The main idea is to fit a large number of models for a small number of epochs and to only continue training for the models achieving the highest accuracy on the validation set. \n",
    "  - max_epochs variable is the max number of epochs that a model can be trained for.\n",
    "- Bayesian optimization is a probabilistic model that maps the hyperparameters to a probability score on the objective function. Unlike Random Search and Hyperband models, Bayesian Optimization keeps track of its past evaluation results and uses it to build the probability model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the hyperparameters optimization algorithm (tuner) with the search method.\n",
    "\n",
    "The search function takes as input the training data and a validation split to perform hyperparameter combinations evaluation. The epochs parameter is used in random search and Bayesian Optimization to define the number of training epochs for each hyperparameter combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(x=train_x, y=train_y, epochs=30, batch_size = 24,\n",
    "             callbacks=[tensorflow.keras.callbacks.EarlyStopping('val_loss', patience=3)], \n",
    "             validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the best model(s) and get a summary of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = best_model.evaluate(test_x, test_y)\n",
    "print(loss)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
